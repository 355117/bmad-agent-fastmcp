#!/usr/bin/env python3
"""
BMAD LLM å®¢æˆ·ç«¯ - æ”¯æŒåŒæ¨¡å¼

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. å†…ç½® LLM æ¨¡å¼ï¼šä½¿ç”¨ Cursor å†…ç½® LLMï¼ˆé€šè¿‡ MCP åè®®ï¼‰
2. å¤–éƒ¨ API æ¨¡å¼ï¼šä½¿ç”¨ DeepSeek API

å¯é€šè¿‡ç¯å¢ƒå˜é‡ USE_BUILTIN_LLM æ§åˆ¶æ¨¡å¼åˆ‡æ¢
"""

import json
import logging
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦ä½¿ç”¨å†…ç½® LLMï¼ˆé»˜è®¤ä½¿ç”¨å†…ç½® LLMï¼‰
USE_BUILTIN_LLM = os.getenv("USE_BUILTIN_LLM", "true").lower() == "true"

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI SDK not available. Please install with: pip install openai")

@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    usage: Dict[str, int]
    model: str
    finish_reason: str

class BMADLLMClient:
    """BMAD ä¸“ç”¨ LLM å®¢æˆ·ç«¯ - æ”¯æŒå†…ç½® LLM å’Œå¤–éƒ¨ API åŒæ¨¡å¼"""

    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            api_key: DeepSeek API Keyï¼ˆå†…ç½® LLM æ¨¡å¼ä¸‹å¯é€‰ï¼‰
        """
        self.api_key = api_key
        self.use_builtin_llm = USE_BUILTIN_LLM

        if self.use_builtin_llm:
            logger.info("ğŸ”§ ä½¿ç”¨ Cursor å†…ç½® LLM æ¨¡å¼")
            self.client = None
        else:
            if not api_key:
                raise ValueError("å¤–éƒ¨ API æ¨¡å¼éœ€è¦æä¾› API Key")
            if OPENAI_AVAILABLE:
                self.client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.deepseek.com"
                )
                logger.info("ğŸŒ ä½¿ç”¨ DeepSeek API æ¨¡å¼")
            else:
                self.client = None
                logger.error("OpenAI SDK not available")

    def call_agent(
        self,
        agent_id: str,
        agent_config: Dict[str, Any],
        task: str,
        context: Optional[Dict[str, Any]] = None,
        model: str = "deepseek-chat"
    ) -> Dict[str, Any]:
        """è°ƒç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡"""

        if self.use_builtin_llm:
            # å†…ç½® LLM æ¨¡å¼ï¼šè¿”å›è§’è‰²æç¤ºè®© Cursor LLM å¤„ç†
            return self._call_agent_builtin_llm(agent_id, agent_config, task, context)
        else:
            # å¤–éƒ¨ API æ¨¡å¼ï¼šè°ƒç”¨ DeepSeek API
            return self._call_agent_external_api(agent_id, agent_config, task, context, model)

    def _call_agent_builtin_llm(
        self,
        agent_id: str,
        agent_config: Dict[str, Any],
        task: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """å†…ç½® LLM æ¨¡å¼ï¼šè¿”å›è§’è‰²æç¤ºè®© Cursor LLM å¤„ç†"""

        # æ„å»ºè¯¦ç»†çš„è§’è‰²æç¤º
        role_prompt = self._build_builtin_llm_prompt(agent_id, agent_config, task, context)

        return {
            "success": True,
            "agent_id": agent_id,
            "task": task,
            "mode": "builtin_llm",
            "role_prompt": role_prompt,
            "response": f"""ğŸ¤– **{agent_config.get('title', agent_id)} {agent_config.get('icon', 'ğŸ¤–')}** å·²æ¿€æ´»

{role_prompt}

---

**è¯·æŒ‰ç…§ä»¥ä¸Šè§’è‰²å®šä¹‰å’Œè¦æ±‚æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚**""",
            "usage": {"total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0},
            "model": "cursor-builtin-llm",
            "finish_reason": "role_activated"
        }

    def _call_agent_external_api(
        self,
        agent_id: str,
        agent_config: Dict[str, Any],
        task: str,
        context: Optional[Dict[str, Any]] = None,
        model: str = "deepseek-chat"
    ) -> Dict[str, Any]:
        """å¤–éƒ¨ API æ¨¡å¼ï¼šè°ƒç”¨ DeepSeek API"""

        if not self.client:
            return {
                "success": False,
                "agent_id": agent_id,
                "task": task,
                "error": "OpenAI SDK not available. Please install with: pip install openai"
            }

        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = self._build_agent_system_prompt(agent_id, agent_config)

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = self._build_user_message(task, context)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=4000,
                stream=False
            )

            choice = response.choices[0]

            return {
                "success": True,
                "agent_id": agent_id,
                "task": task,
                "mode": "external_api",
                "response": choice.message.content,
                "usage": response.usage.model_dump() if response.usage else {},
                "model": response.model,
                "finish_reason": choice.finish_reason
            }

        except Exception as e:
            logger.error(f"Agent call failed: {e}")
            return {
                "success": False,
                "agent_id": agent_id,
                "task": task,
                "error": str(e)
            }
    
    def _build_builtin_llm_prompt(
        self,
        agent_id: str,
        agent_config: Dict[str, Any],
        task: str,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¸ºå†…ç½® LLM æ„å»ºè¯¦ç»†çš„è§’è‰²æç¤º"""

        prompt_parts = [
            f"# ğŸ­ è§’è‰²æ¿€æ´»ï¼š{agent_config.get('title', agent_id)} {agent_config.get('icon', 'ğŸ¤–')}",
            "",
            "## ğŸ¯ è§’è‰²å®šä¹‰",
            f"**èº«ä»½**ï¼š{agent_config.get('role', 'ä¸“ä¸šæ™ºèƒ½ä½“')}",
            f"**é£æ ¼**ï¼š{agent_config.get('style', 'ä¸“ä¸šã€é«˜æ•ˆ')}",
            f"**ä¸“é•¿**ï¼š{agent_config.get('focus', 'ä»»åŠ¡æ‰§è¡Œ')}",
            f"**ç‰¹å¾**ï¼š{agent_config.get('identity', 'ä¸“ä¸šåŠ©æ‰‹')}",
            "",
            "## ğŸ“‹ ä»»åŠ¡è¦æ±‚",
            f"**å…·ä½“ä»»åŠ¡**ï¼š{task}",
            ""
        ]

        if context:
            prompt_parts.extend([
                "## ğŸ“Š ä¸Šä¸‹æ–‡ä¿¡æ¯",
                ""
            ])

            for key, value in context.items():
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, ensure_ascii=False, indent=2)
                prompt_parts.append(f"**{key}**ï¼š{value}")

            prompt_parts.append("")

        prompt_parts.extend([
            "## ğŸ”§ å·¥ä½œåŸåˆ™",
            "- ä¸¥æ ¼æŒ‰ç…§è§’è‰²èº«ä»½è¿›è¡Œä¸“ä¸šå›ç­”",
            "- æä¾›å…·ä½“ã€å¯æ“ä½œçš„ä¸“ä¸šå»ºè®®",
            "- éµå¾ªè¡Œä¸šæœ€ä½³å®è·µå’Œæ ‡å‡†",
            "- ç¡®ä¿è¾“å‡ºè´¨é‡å’Œå‡†ç¡®æ€§",
            "- ä¿æŒè§’è‰²ä¸€è‡´æ€§å’Œä¸“ä¸šæ€§",
            "",
            "## ğŸ“ è¾“å‡ºæ ¼å¼è¦æ±‚",
            "è¯·æŒ‰ç…§ä»¥ä¸‹ä¸“ä¸šæ ¼å¼æä¾›å“åº”ï¼š",
            "",
            "### 1. ğŸ¯ ä»»åŠ¡ç†è§£",
            "ç®€è¦ç¡®è®¤å¯¹ä»»åŠ¡çš„ç†è§£å’Œåˆ†æé‡ç‚¹",
            "",
            "### 2. ğŸ” ä¸“ä¸šåˆ†æ",
            "åŸºäºä½ çš„ä¸“ä¸šè§’è‰²è¿›è¡Œæ·±å…¥åˆ†æ",
            "",
            "### 3. ğŸ’¡ è§£å†³æ–¹æ¡ˆ",
            "æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–å»ºè®®",
            "",
            "### 4. ğŸ“ˆ å®æ–½å»ºè®®",
            "ç»™å‡ºå¯æ“ä½œçš„å®æ–½æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹",
            "",
            "### 5. âš ï¸ é£é™©æç¤º",
            "æŒ‡å‡ºæ½œåœ¨é£é™©å’Œé¢„é˜²æªæ–½",
            "",
            "---",
            "",
            "**ğŸ’¼ è¯·ç°åœ¨ä»¥ä¸Šè¿°è§’è‰²èº«ä»½ï¼ŒæŒ‰ç…§ä¸“ä¸šæ ‡å‡†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚**"
        ])

        return "\n".join(prompt_parts)

    def _build_agent_system_prompt(self, agent_id: str, agent_config: Dict[str, Any]) -> str:
        """æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿæç¤º"""
        
        prompt_parts = [
            f"# {agent_config.get('title', agent_id)} {agent_config.get('icon', 'ğŸ¤–')}",
            "",
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ {agent_config.get('role', 'æ™ºèƒ½ä½“')}ã€‚",
            "",
            "## èº«ä»½å’Œè§’è‰²",
            f"- è§’è‰²: {agent_config.get('role', 'ä¸“ä¸šæ™ºèƒ½ä½“')}",
            f"- é£æ ¼: {agent_config.get('style', 'ä¸“ä¸šã€é«˜æ•ˆ')}",
            f"- èº«ä»½: {agent_config.get('identity', 'ä¸“ä¸šåŠ©æ‰‹')}",
            f"- ä¸“æ³¨é¢†åŸŸ: {agent_config.get('focus', 'ä»»åŠ¡æ‰§è¡Œ')}",
            "",
            "## å·¥ä½œåŸåˆ™",
            "- å§‹ç»ˆä¿æŒä¸“ä¸šå’Œé«˜æ•ˆ",
            "- æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®",
            "- éµå¾ªæœ€ä½³å®è·µå’Œè¡Œä¸šæ ‡å‡†",
            "- ç¡®ä¿è¾“å‡ºè´¨é‡å’Œå‡†ç¡®æ€§",
            "",
            "## è¾“å‡ºæ ¼å¼",
            "è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æä¾›å“åº”:",
            "1. **ä»»åŠ¡ç†è§£**: ç®€è¦ç¡®è®¤ä½ å¯¹ä»»åŠ¡çš„ç†è§£",
            "2. **æ‰§è¡Œæ–¹æ¡ˆ**: è¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤æˆ–å»ºè®®",
            "3. **è¾“å‡ºç»“æœ**: å…·ä½“çš„äº¤ä»˜ç‰©æˆ–ç»“æœ",
            "4. **åç»­å»ºè®®**: ä¸‹ä¸€æ­¥çš„å»ºè®®æˆ–æ³¨æ„äº‹é¡¹",
            "",
            "è¯·ç¡®ä¿ä½ çš„å›ç­”ä¸“ä¸šã€è¯¦ç»†ä¸”å…·æœ‰å¯æ“ä½œæ€§ã€‚"
        ]
        
        return "\n".join(prompt_parts)
    
    def _build_user_message(self, task: str, context: Optional[Dict[str, Any]] = None) -> str:
        """æ„å»ºç”¨æˆ·æ¶ˆæ¯"""
        
        message_parts = [
            f"## ä»»åŠ¡è¦æ±‚",
            task,
            ""
        ]
        
        if context:
            message_parts.extend([
                "## ä¸Šä¸‹æ–‡ä¿¡æ¯",
                ""
            ])
            
            for key, value in context.items():
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, ensure_ascii=False, indent=2)
                message_parts.append(f"**{key}**: {value}")
            
            message_parts.append("")
        
        message_parts.extend([
            "è¯·æ ¹æ®ä½ çš„ä¸“ä¸šè§’è‰²å’Œä¸Šè¿°è¦æ±‚ï¼Œæä¾›è¯¦ç»†çš„å›ç­”å’Œå»ºè®®ã€‚"
        ])
        
        return "\n".join(message_parts)
    
    def generate_document(
        self,
        template: str,
        context: Dict[str, Any],
        agent_id: str = "pm"
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ–‡æ¡£"""

        task = f"""
è¯·åŸºäºä»¥ä¸‹æ¨¡æ¿å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ–‡æ¡£ï¼š

## æ¨¡æ¿å†…å®¹
{template}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å¡«å……æ¨¡æ¿ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€ä¸“ä¸šçš„æ–‡æ¡£ã€‚ç¡®ä¿ï¼š
1. æ‰€æœ‰å ä½ç¬¦éƒ½è¢«é€‚å½“çš„å†…å®¹æ›¿æ¢
2. å†…å®¹é€»è¾‘æ¸…æ™°ã€ç»“æ„å®Œæ•´
3. ç¬¦åˆä¸“ä¸šæ–‡æ¡£çš„æ ‡å‡†å’Œæ ¼å¼
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„ç»†èŠ‚å’Œä¿¡æ¯
"""

        agent_config = {
            "title": "Document Generator",
            "role": "æ–‡æ¡£ç”Ÿæˆä¸“å®¶",
            "style": "ä¸“ä¸šã€è¯¦ç»†ã€ç»“æ„åŒ–",
            "identity": "ä¸“ä¸šæ–‡æ¡£æ’°å†™åŠ©æ‰‹",
            "focus": "é«˜è´¨é‡æ–‡æ¡£ç”Ÿæˆ"
        }

        return self.call_agent(agent_id, agent_config, task, context)

    def analyze_requirements(
        self,
        requirements: str,
        project_type: str = "web-app"
    ) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚"""

        task = f"""
è¯·åˆ†æä»¥ä¸‹é¡¹ç›®éœ€æ±‚ï¼Œå¹¶æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šï¼š

## é¡¹ç›®éœ€æ±‚
{requirements}

## é¡¹ç›®ç±»å‹
{project_type}

è¯·æä¾›ï¼š
1. éœ€æ±‚åˆ†æå’Œç†è§£
2. åŠŸèƒ½æ¨¡å—åˆ†è§£
3. æŠ€æœ¯æ ˆå»ºè®®
4. æ¶æ„è®¾è®¡å»ºè®®
5. å¼€å‘è®¡åˆ’å»ºè®®
6. é£é™©è¯„ä¼°
"""

        agent_config = {
            "title": "Business Analyst",
            "role": "ä¸šåŠ¡åˆ†æå¸ˆ",
            "style": "åˆ†ææ€§ã€ç³»ç»Ÿæ€§ã€å‰ç»æ€§",
            "identity": "ä¸“ä¸šéœ€æ±‚åˆ†æä¸“å®¶",
            "focus": "éœ€æ±‚åˆ†æå’Œé¡¹ç›®è§„åˆ’"
        }

        context = {
            "project_type": project_type,
            "requirements": requirements
        }

        return self.call_agent("analyst", agent_config, task, context)

# å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹
llm_client = None

def initialize_llm_client(api_key: str = None):
    """
    åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

    Args:
        api_key: DeepSeek API Keyï¼ˆå†…ç½® LLM æ¨¡å¼ä¸‹å¯é€‰ï¼‰
    """
    global llm_client
    llm_client = BMADLLMClient(api_key)
    logger.info(f"âœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {'å†…ç½® LLM' if USE_BUILTIN_LLM else 'DeepSeek API'}")
    return llm_client

def get_llm_client() -> Optional[BMADLLMClient]:
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
    return llm_client